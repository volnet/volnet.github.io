机器学习（Machine Learning）
==============================

![](contents/cover.jpg)

作者：[英]Peter Flach 著

段菲 译

The Art and Science of Algorithms that Make Sense of Data

推荐序
------------------------------

本书对经典机器学习框架中的模型做了非常系统的梳理和分类，涵盖了机器学习基础知识的主要部分。

序
------------------------------

作为一本导论性质的读物，作者想要保持机器学习各个领域的统一性和多样性：

- 统一性：通过将“任务”和“特征”分开处理。
- 多样性：覆盖大量逻辑模型、几何模型和概率模型。

机器学习是统计学和知识表示“联姻”的产物。

绪论 机器学习概述
------------------------------

作者从一个垃圾邮件过滤器（[SpamAssassin](http://spamassassin.apache.org)）入手来讲解机器学习。从而给出了机器学习的定义：

> 机器学习是对依据经验提升自身性能或丰富自身知识的各种算法和系统的系统性研究。

从SpamAssassin的例子中，“经验”对应一组正确标注的训练数据，而“性能”对应识别垃圾邮件的能力。

作者使用了线性分类的方式，讲解了SpamAssassin的分类规则。

一个机器学习问题往往存在多种解决方案，同时可能引出多种问题：

- 可能1，过拟合：在一味追求系统在训练数据上的性能，很容易造成一种貌似喜人、实则存在巨大隐患的现象——[过拟合（overfitting）](https://en.wikipedia.org/wiki/Overfitting)。（类似死记硬背某种答案无法真正灵活运用某种知识）
- 可能2，训练数据不符合实际情况：更换训练数据可以改变这种结果。但是也有可能无法找到合适的训练数据，可以用下面两个方法来处理：
    - 1、可能其中某些数据产生了噪声，可以标记为无视它们。
    - 2、尝试描述能力更强的分类器（比如添加一条新的规则共同决策）

除了线性分类，作者又使用了贝叶斯文本分类，来完成相同的任务：通过概率论，估算关键字在文本分类器的词汇表中出现的概率，来判定是否是垃圾邮件。贝叶斯分类的好处是进一步的依据可在原有的基础上使用（如一封邮件同时命中了两个垃圾邮件关键词，那么它是垃圾邮件的概率也将提升）。

然后作者又介绍了基于规则的案例，它将可能独立出现，和可能同时出现的词进行了区分，比如一个“伟哥”和“蓝色药片”可能会同时出现，但是和“彩票”通常不会同时出现，如果单纯使用贝叶斯文本分类方法可能会将同时出现的场景重复计算两次，而基于规则的判定，会仅在“伟哥”的后面才会判断“蓝色药片”出现的概率。

上面这些为了将垃圾邮件分为两类的方法，统称为_二元分类_或_两类分类_(binary classification)。要完成该任务，需要将每封电子邮件转化为一组变量或特征。

通过分析带有正确标注信息的电子邮件训练集，发现样本特征与其所属类别之间的联系，这种联系通常称为_模型_。

作者通过对线性分类、贝叶斯文本分类、规则分类进行了介绍，讲述了机器学习的一些基本的概念。

任务、模型及特征是机器学习的三大“原料”。下面的图是运用机器学习解决给定问题的示意：

![](contents/chapter00/diagram-02.png)

要完成一项任务，需要建立从用特征描述的数据到输出的恰当映射（即模型）。学习问题的中心任务就是研究如何从训练数据中获取这样的映射。

同时列出SpamAssassin的相同模型来参考：

![](contents/chapter00/diagram-01.png)

机器学习所关注的问题是使用正确的特征来构建正确的模型，以完成既定的任务。

第1章 机器学习的构成要素
------------------------------

一旦获得对问题域中对象的某种恰当的特征表示，我们往往不必再去关注这些对象本身。

许多任务都可抽象为一个从数据点到输出的映射，而这种映射或模型本身又是应用于训练数据的某个机器学习算法的输出。

模型赋予了机器学习领域以多样性，而特征和任务则为其带来了某种程度的一致性。

### 1.1 任务：可通过机器学习解决的问题

回归（regression），其本质上是依据标注有函数输出真值的训练样本集来学习一个实值函数。

从无标注的数据中学习称为无监督学习（unsupervised learning），与之相对的是有监督学习（supervised learning），二者的不同之处在于后者必须借助带有标注信息的训练数据。

典型的聚类算法通常是首先计算不同实例（即聚类的对象，如电子邮件）之间的相似性，然后将那些“相似”的实例放入同一个簇（cluster）中，而将“不相似”的实例放入不同的簇中。

#### 1.1.1 探寻结构

作者将观众对于4部影片的评分，用一个6x4的矩阵来表示。而评分实际上是由多个维度综合的结果，作者假定为题材、决定观众对影片的偏好时，犯罪题材所起的作用为其他两种题材的两倍、影片与类型的关系三个维度。三者相乘的结果正是6x4的矩阵。

我们还可以按照模型的输出是否含有目标变量来划分模型：如果有，则称其为预测性模型（predicative model）；否则称其为描述性模型（descriptive model）。

|          | 预测性模型 | 描述性模型              |
|:--------:|:---------:|:--------------------:|
| 有监督学习 | 分类、回归 | 子群发现               | 
| 无监督学习 | 预测性聚类 | 描述性聚类、关联规则发现 |

除了上文中的四种设置外，还有一种模型值得一提，即预测性模型的半监督学习（semisupervised learning）。在许多问题域中，数据虽易于获取，但要得到有标注的数据则代价不菲。利用半监督学习，可以在一定程度上缓解这种矛盾。一种可行的方案是首先依据少量带标注的训练样本来构造一个初始模型，然后利用无标注的数据对模型进行优化。

#### 1.1.2 性能评价

传统的算法，如排序算法，结果都是固定的，比如不同的排序算法，虽然性能上有差异，但是排序结果是一致的。但是机器学习的算法则截然不同，因为数据中可能夹杂着噪音，过度追求能处理所有数据的算法，可能会导致模型过拟合。我们需要获悉算法在新数据上预期的分类性能（假定我们面临的是一个分类任务）如何，而并非只从运行时间或内存占用来考虑，虽然这些问题也很重要。

分类问题中，将分类正确的数量除以总数，得出分类器的准确率。但它却不能表明是否出现了过拟合现象。一种更好的思路是仅抽取90%的数据用于训练，剩下的10%数据作为测试集，如果在两个集合上的性能十分相似，则可以避免过拟合的问题。但是如此选出的90%和10%有运气的成分，即便用了随机选取，也仍然无法避免。可以采用交叉验证的方法，将数据分成10份，每次取其中一份（10%）作为测试集，其他数据做训练集，重复10次，最后对模型的测试集（共10个）性能取平均值。

交叉验证也可以应用于其他有监督学习问题，但无监督学习问题则要采用另外一套评价方法。

### 1.2 模型：机器学习的输出

模型是为了解决某个既定问题而从数据中学习到的，因此它是机器学习中最核心的概念。

#### 1.2.1 几何模型

几何模型是借助于一些几何概念（如线、平面及距离）直接在实例空间中构建的。

那些有可能运用高维空间的几何概念通常都带有前缀“超”（hyper-）。

如果存在某个线性决策面能够将两类样本分离，则称所给数据集是线性可分的（linearly seperable）。

基本线性分类器（basic linear classifier）的优点在于简单，因为它仅涉及加法、减法及对样本的比例变换。

由于分类的维度可能会有很多，所以就会有很多的决策面，如何从无限种可能的决策面中进行选择？一种很自然的选择是优先考虑能够产生“大间隔”（large margin）的分类器，这里的“间隔”（margin）表示决策面与距其最近的实例之间的距离。

我们希望大多数（即便不是全部）机器学习算法具有：

- 平移不变性（translation-invariant）：对坐标系原点的选取不敏感。
- 旋转不变性（rotation-invariant）：如线性分类器或支持向量机（support vector machine，追求的即是一个能够尽可能大的分类间隔的决策面）

机器学习中另外一个极为有用的几何概念是距离（distance）。如果两个实例之间的距离很小，则意味着二者的特征值相似。

基于距离的分类器：

- 最近邻分类器（nearest-neighbour classifier）：虽然有多种改进方案，但预测具有局部性。
- K均值聚类（K-means）

#### 1.2.2 概率模型



#### 1.2.3 逻辑模型

#### 1.2.4 分组模型与评分模型

第2章 两类分类及相关任务
------------------------------

第3章 超越两类分类
------------------------------

第4章 概念学习
------------------------------

第5章 树模型
------------------------------

第6章 规则模型
------------------------------

第7章 线性模型
------------------------------

第8章 基于距离的模型
------------------------------

第9章 概率模型
------------------------------

第10章 特征
------------------------------

第11章 模型的集成
------------------------------

第12章 机器学习的实验
------------------------------

后记 路在何方
------------------------------

记忆要点
------------------------------

参考文献
------------------------------